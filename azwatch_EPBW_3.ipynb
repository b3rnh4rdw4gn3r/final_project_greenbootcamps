
# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier  # Ensure xgboost is installed
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.cluster import KMeans, DBSCAN

# Specify the file path
file_path = "/mnt/data/AZWatch_subscribers.csv"

# Read the CSV file into a DataFrame
df = pd.read_csv(file_path)

# Convert string labels to numerical values
label_mapping = {'churned': 0, 'subscribed': 1}
df['subscription_status'] = df['subscription_status'].map(label_mapping)

# Display basic information
print("First five rows of the dataset:")
print(df.head())

print("\nGeneral information about the dataset:")
print(df.info())

print("\nStatistical summary of the dataset:")
print(df.describe())

# Check for missing values
print("\nNumber of missing values in each column:")
print(df.isnull().sum())

# One-Hot Encoding for the categorical attribute: 'age_group'
df_encoded = pd.get_dummies(df, columns=['age_group'])

# Separate predictor variables from class label
X = df_encoded.drop(['subscriber_id', 'subscription_status'], axis=1)
y = df_encoded['subscription_status']

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Data processing: Apply Standard Scaling on numerical attributes
scaler = StandardScaler()
X_train_prepared = X_train.copy()
X_test_prepared = X_test.copy()
X_train_prepared[['engagement_time', 'engagement_frequency']] = scaler.fit_transform(X_train[['engagement_time', 'engagement_frequency']])
X_test_prepared[['engagement_time', 'engagement_frequency']] = scaler.transform(X_test[['engagement_time', 'engagement_frequency']])

# Define models
models = {
    "Logistic Regression": LogisticRegression(),
    "Decision Tree": DecisionTreeClassifier(max_depth=3, criterion="gini"),
    "Random Forest": RandomForestClassifier(n_estimators=10, max_depth=3),
    "Gradient Boosting": GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=42),
    "XGBoost": XGBClassifier(eval_metric='logloss')
}

# Train and evaluate each model
results = []

for name, model in models.items():
    model.fit(X_train_prepared, y_train)
    y_pred = model.predict(X_test_prepared)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    results.append({
        "Model": name,
        "Accuracy": accuracy,
        "Precision": precision,
        "Recall": recall,
        "F1 Score": f1
    })
    print(f"\n{name} classification report:")
    print(classification_report(y_test, y_pred))

# Convert results to DataFrame for better visualization
results_df = pd.DataFrame(results)
print("\nSummary of model performance:")
print(results_df)

# Prepare the data for clustering including 'age_group'
segmentation = df_encoded.drop(['subscriber_id', 'subscription_status'], axis=1)

# Scale the numerical data attributes to standardize the data
scaler = StandardScaler()
segmentation[['engagement_time', 'engagement_frequency']] = scaler.fit_transform(segmentation[['engagement_time', 'engagement_frequency']])

# Determine the optimal number of clusters using the elbow method
sse = {}  # Dictionary to store the sum of squared errors for each value of k
for k in range(1, 20):
    kmeans = KMeans(n_clusters=k, random_state=1)
    kmeans.fit(segmentation)
    sse[k] = kmeans.inertia_  # Inertia is the sum of squared distances to the nearest cluster center

# Plot the SSE for each value of k to visualize the elbow point (optional)
# plt.figure(figsize=(8, 6))
# sns.pointplot(x=list(sse.keys()), y=list(sse.values()), color='teal')
# plt.title('Elbow Method to Choose k', fontsize=15)
# plt.xlabel('Number of Clusters (k)', fontsize=12)
# plt.ylabel('SSE', fontsize=12)
# plt.show()

# Apply K-Means clustering with 3 clusters (chosen based on the elbow method)
kmeans = KMeans(n_clusters=3, random_state=1)
segmentation['cluster_id'] = kmeans.fit_predict(segmentation)

# Analyze the average feature values and counts per cluster
analysis = segmentation.groupby(['cluster_id']).mean().round(0)
print("\nK-means clustering analysis:")
print(analysis)

# ADDITIONAL CLUSTERING METHOD: DBSCAN
# DBSCAN clustering to identify clusters based on density
# DBSCAN is effective for datasets with noise and clusters of varying shapes
db = DBSCAN(eps=0.5, min_samples=5).fit(segmentation)
labels = db.labels_

# Add DBSCAN cluster labels (including noise points labeled as -1) to the segmentation DataFrame
segmentation['dbscan_cluster_id'] = labels

# Analyze the average feature values and counts per DBSCAN cluster
dbscan_analysis = segmentation.groupby(['dbscan_cluster_id']).mean().round(0)
print("\nDBSCAN clustering analysis:")
print(dbscan_analysis)
